# -*- coding: utf-8 -*-
"""evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Lukeasargen/GarbageML/blob/main/evaluation.ipynb
"""

import time

import matplotlib.pyplot as plt
from pytorch_lightning.metrics.functional import accuracy, precision_recall, f1
import seaborn as sn
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import torch
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
from torchvision import transforms as T

from model import GarbageModel
from util import make_ensemble, time_to_string

model_paths = [
        # "logs/subset/version_87/last.ckpt",  # mobilenet_v3_small.attn
        "logs/subset/version_91/last.ckpt",  # resnet50.attn
]
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print("Device : {}".format(device))
model = make_ensemble(model_paths, GarbageModel, device)

# root = "data/test448"
root = r"C:\Users\LUKE_SARGEN\projects\datasets\test512"
batch_size = 224
use_crops = False
input_size = 224  # model.input_size, 128, 192, 224, 256, 288, 320, 384, 448, 512

if use_crops:
    valid_transform = T.Compose([
        T.Resize(int(1.05*input_size)),
        T.FiveCrop(input_size),
        # T.TenCrop(input_size),
        T.Lambda(lambda crops: torch.stack([T.ToTensor()(crop) for crop in crops])),
    ])
else:
    valid_transform = T.Compose([
        T.Resize(input_size, interpolation=T.InterpolationMode.BICUBIC),
        T.CenterCrop(input_size),
        T.ToTensor(),
    ])
valid_ds = ImageFolder(root=root, transform=valid_transform)
num_classes = len(valid_ds.classes)
val_loader = DataLoader(dataset=valid_ds, batch_size=batch_size, shuffle=False, num_workers=0)
print("{} Test Samples.".format(len(valid_ds)))

torch.cuda.empty_cache()
yhats = []  # All model predictions
ensemble = []  # Mean prediction
targets = []  # Targets
t0 = time.time()
i = 0
for (data, target) in val_loader:
    data = data.to(device)
    if use_crops:
        bs, ncrops, c, h, w = data.size()
        data = data.view(-1, c, h, w)
        with torch.no_grad():
            ymodels = [torch.softmax(m(data.clone()), dim=1).view(bs, ncrops, -1).mean(1) for m in model.models]
        avg_pred = torch.mean(torch.stack(ymodels), dim=0)
        yhat = torch.stack([torch.argmax(y, dim=1) for y in ymodels])
    else:
        with torch.no_grad():
            ymodels = [torch.softmax(m(data.clone()), dim=1) for m in model.models]
        avg_pred = torch.mean(torch.stack(ymodels), dim=0)
        yhat = torch.stack([torch.argmax(y, dim=1) for y in ymodels])
    yhats.append(yhat)
    ensemble.append(avg_pred)
    targets.append(target)
    if i%max(1, int(len(valid_ds)/(batch_size*10)))==0:
        c = len(targets)*batch_size
        duration = time.time()-t0
        remaining = duration/c * (len(valid_ds)-c)
        print("image {}/{}. {:.1f} images/second. elapsed={}. remaining={}.".format(c, len(valid_ds), c/duration, time_to_string(duration), time_to_string(remaining)))
    i += 1

preds = torch.cat(yhats, dim=1).cpu()
pred = torch.argmax(torch.cat(ensemble), dim=1).cpu()
target = torch.cat(targets).cpu()
c = len(target)
duration = time.time()-t0
remaining = duration/c * (len(valid_ds)-c)
print("image {}/{}. {:.1f} images/second. elapsed={}.".format(c, len(valid_ds), c/duration, time_to_string(duration)))
print("preds.shape :", preds.shape)
print("pred.shape :", pred.shape)
print("target.shape :", target.shape)

def stats(pred, target):
    acc = accuracy(pred, target)
    avg_precision, avg_recall = precision_recall(pred, target, num_classes=num_classes, average="macro", mdmc_average="global")
    weighted_f1 = f1(pred, target, num_classes=num_classes, threshold=0.5, average="weighted")
    return 100*acc, 100*avg_precision, 100*avg_recall, 100*weighted_f1

archs = [m.hparams.model for m in model.models]
headers = ["Model", "Arch", "Accuracy", "Precision", "Recall", "F1 Score"]
hw = max(len(n) for n in headers)
aw = max(len(n) for n in archs)
width = max(len(n) for n in headers)
head_fmt = '{:>{hw}s}' + ' {:>{aw}s}' + ' {:>{hw}s}'*(len(headers)-2) + '\n'
row_fmt = '{:>{hw}s}' + ' {:>{aw}s}' + ' {:>9.2f}'*(len(headers)-2) + '\n'
stat_str = head_fmt.format(*headers, hw=hw, aw=aw)
for i in range(len(model.models)):
    row = stats(preds[i, :], target)
    stat_str += row_fmt.format(str(i), str(archs[i]),*row, hw=hw, aw=aw)
row = stats(pred, target)
stat_str += row_fmt.format("Ensemble", "", *row, hw=hw, aw=aw)
print(stat_str)

print( classification_report(target, pred, target_names=model.classes, digits=4) )

cm = confusion_matrix(target, pred)
plt.figure(figsize=(8, 8))
sn.set(font_scale=1.1)  # Label size
sn.heatmap(cm, annot=True, fmt="d",
cmap='Blues',
# cbar=False,
vmin=0,
xticklabels=model.classes,  yticklabels=model.classes, annot_kws={"size": 16}, square=True, linewidths=0.5)
plt.title("Confusion Matrix")
plt.ylabel('True label')
plt.xlabel('Predicted label\n\nAccuracy={:.2f}\nPrecision={:.2f}\nRecall={:.2f}\nF1 Score={:.2f}'.format(*row))
plt.savefig('data/confusion_matrix.png', dpi=150, bbox_inches='tight')
plt.show()

